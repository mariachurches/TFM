{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariachurches/TFM/blob/main/segmentation/DeepLabv3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byfJ_8zKXLdY"
      },
      "source": [
        "1. Montamos google drive para tener acceso a las imagenes\n",
        "2. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BwM-SfFTYLRI"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "import PIL\n",
        "import torch\n",
        "import random\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from statistics import mean\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms as T\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.transforms.functional import InterpolationMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r-gsvcH5RTV",
        "outputId": "a471c8b2-3aff-4900-fc6b-ad303449794b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BuLYupHXXrmW"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('/content/Escoliosis/labels_files'):\n",
        "  !unzip /content/drive/MyDrive/TFM/Escoliosis.zip -d . &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HxeAGrmXgKJ3"
      },
      "outputs": [],
      "source": [
        "#Creamos variables que contengan los directorios a utilizar\n",
        "IMAGES_PATH = \"/content/Escoliosis/orig_img\"\n",
        "MASKS_PATH = \"/content/Escoliosis/segm_img\"\n",
        "LABELS_PATH     = \"/content/Escoliosis/labels_files\"\n",
        "OUTPUT_IMAGES_PATH     = \"/content/drive/MyDrive/TFM/results/\"\n",
        "OUTPUT_MODEL_PATH     = \"/content/drive/MyDrive/TFM/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EdpD-CHZX67X"
      },
      "outputs": [],
      "source": [
        "labels_files   = os.listdir(LABELS_PATH)\n",
        "images_dataset = os.listdir(IMAGES_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRkfgH_VBxqa",
        "outputId": "4bd87f4f-6c02-466f-aaed-eca3c139ff72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hay un total de 609 imagenes \n"
          ]
        }
      ],
      "source": [
        "images_dataset = [x for x in images_dataset if \"jpg\" in x]\n",
        "print(f\"Hay un total de {len(images_dataset)} imagenes \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KRHv7niLxbcT"
      },
      "outputs": [],
      "source": [
        "angles = []\n",
        "for cvs_l in labels_files:\n",
        "  with open(os.path.join(LABELS_PATH, cvs_l), newline='') as File:  \n",
        "      reader = csv.reader(File)\n",
        "      for row in reader:\n",
        "          angles.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jkTaNAzwz7LA"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(images_dataset, angles, test_size=0.1, shuffle = True, random_state = 8)\n",
        "X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.22, random_state= 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CliaUyKGkzd6"
      },
      "outputs": [],
      "source": [
        "shapes = [cv2.imread(os.path.join(IMAGES_PATH,x),0).shape for x in images_dataset]\n",
        "K=0\n",
        "height =  (int(mean([val[K] for val in shapes])) // 8) * 8\n",
        "K=1\n",
        "width = (int(mean(val[K] for val in shapes)) // 8) * 8\n",
        "\n",
        "height=800\n",
        "width=500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wotSAF9dysFI"
      },
      "outputs": [],
      "source": [
        "class my_own_transformation(object):\n",
        "\n",
        "    def __call__(self, img):\n",
        "        ret, bw_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
        "        return bw_img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"custom transformation for masks\"\n",
        "\n",
        "class __img_to_tensor(object):\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = np.asarray(img)\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        img = torch.from_numpy(img)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"custom transformation for masks\"\n",
        "\n",
        "class my_normalize(object):\n",
        "    '''Normalize image'''\n",
        " \n",
        "    def __call__(self, img):\n",
        "        return img.type(torch.FloatTensor)/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PfszMeLFyBz2"
      },
      "outputs": [],
      "source": [
        "transforms_img   = T.Compose([T.ToPILImage(), T.Resize((height,width), interpolation=InterpolationMode.NEAREST), T.ToTensor(), T.Lambda(my_normalize())])\n",
        "transforms_masks = T.Compose([T.Lambda(my_own_transformation()), T.ToPILImage(), T.Resize((height,width), interpolation=InterpolationMode.NEAREST), T.Lambda(__img_to_tensor()), T.Lambda(my_normalize())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVpfno2YP7qk",
        "outputId": "2cbf2036-db6e-414b-8e67-d69a3d965846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "426 121 62\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 2\n",
        "TRAIN_SIZE = int(len(images_dataset)*0.7)\n",
        "VAL_SIZE = int(len(images_dataset) * 0.2)\n",
        "TEST_SIZE = len(images_dataset) - TRAIN_SIZE - VAL_SIZE\n",
        "print(TRAIN_SIZE, VAL_SIZE, TEST_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HLMGWdNUxmdZ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Any, Callable, Optional\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "\n",
        "\n",
        "class SegmentationDataset(VisionDataset):\n",
        "    \"\"\"A PyTorch dataset for image segmentation task.\n",
        "    The dataset is compatible with torchvision transforms.\n",
        "    The transforms passed would be applied to both the Images and Masks.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 image_folder: str,\n",
        "                 mask_folder: str,\n",
        "                 transforms: Optional[Callable] = None,\n",
        "                 seed: int = None,\n",
        "                 fraction: float = None,\n",
        "                 subset: str = None,\n",
        "                 image_color_mode: str = \"rgb\",\n",
        "                 mask_color_mode: str = \"grayscale\") -> None:\n",
        "\n",
        "        image_folder_path = image_folder\n",
        "        mask_folder_path = mask_folder\n",
        "\n",
        "        self.image_color_mode = image_color_mode\n",
        "        self.mask_color_mode = mask_color_mode\n",
        "\n",
        "        self.fraction = fraction\n",
        "        self.image_list = np.array(sorted(image_folder_path.glob(\"*\")))\n",
        "        self.mask_list = np.array(sorted(mask_folder_path.glob(\"*\")))\n",
        "        if seed:\n",
        "            np.random.seed(seed)\n",
        "            indices = np.arange(len(self.image_list))\n",
        "            np.random.shuffle(indices)\n",
        "            self.image_list = self.image_list[indices]\n",
        "            self.mask_list = self.mask_list[indices]\n",
        "        if subset == \"Train\":\n",
        "            self.image_names = self.image_list[:int(\n",
        "                    np.ceil(len(self.image_list) * (1 - self.fraction)))]\n",
        "            self.mask_names = self.mask_list[:int(\n",
        "                    np.ceil(len(self.mask_list) * (1 - self.fraction)))]\n",
        "        else:\n",
        "            self.image_names = self.image_list[\n",
        "                    int(np.ceil(len(self.image_list) * (1 - self.fraction))):]\n",
        "            self.mask_names = self.mask_list[\n",
        "                    int(np.ceil(len(self.mask_list) * (1 - self.fraction))):]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Any:\n",
        "        image_path = self.image_names[index]\n",
        "        mask_path = self.mask_names[index]\n",
        "        with open(image_path, \"rb\") as image_file, open(mask_path,\n",
        "                                                        \"rb\") as mask_file:\n",
        "            image = Image.open(image_file)\n",
        "            if self.image_color_mode == \"rgb\":\n",
        "                image = image.convert(\"RGB\")\n",
        "            elif self.image_color_mode == \"grayscale\":\n",
        "                image = image.convert(\"L\")\n",
        "            mask = Image.open(mask_file)\n",
        "            if self.mask_color_mode == \"rgb\":\n",
        "                mask = mask.convert(\"RGB\")\n",
        "            elif self.mask_color_mode == \"grayscale\":\n",
        "                mask = mask.convert(\"L\")\n",
        "            sample = {\"image\": image, \"mask\": mask}\n",
        "            if self.transforms:\n",
        "                sample[\"image\"] = self.transforms(sample[\"image\"])\n",
        "                sample[\"mask\"] = self.transforms(sample[\"mask\"])\n",
        "            return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VmMLMi2XvM0O"
      },
      "outputs": [],
      "source": [
        "\"\"\" DeepLabv3 Model download and change the head for your prediction\"\"\"\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "def createDeepLabv3(outputchannels=1):\n",
        "    \"\"\"DeepLabv3 class with custom head\n",
        "    Args:\n",
        "        outputchannels (int, optional): The number of output channels\n",
        "        in your dataset masks. Defaults to 1.\n",
        "    Returns:\n",
        "        model: Returns the DeepLabv3 model with the ResNet101 backbone.\n",
        "    \"\"\"\n",
        "    # model = models.segmentation.deeplabv3_resnet101(pretrained=True,\n",
        "                                                    # progress=True)\n",
        "    model = models.segmentation.deeplabv3_resnet50(pretrained=True,\n",
        "                                                    progress=True)\n",
        "    model.classifier = DeepLabHead(2048, outputchannels)\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Cj8CpnKRStuw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/TFM/My_modules/')\n",
        "from Nets2 import UNet\n",
        "from Datasets import Spine_Dataset_Segmentation\n",
        "import Nets\n",
        "from Utils import Utils as utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zlTzJBh8wdrJ"
      },
      "outputs": [],
      "source": [
        "class NetManager:\n",
        "    def __init__(self):\n",
        "        self.model = createDeepLabv3()\n",
        "        self.model.train()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0001, betas=(0.9, 0.999), weight_decay=0)\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda:0\")\n",
        "        self.model.to(self.device)\n",
        "        self.print_every = 50\n",
        "\n",
        "    def load(self, filename):\n",
        "        if os.path.isfile(filename):\n",
        "            print(\"=> loading checkpoint '{}'\".format(filename))\n",
        "            checkpoint = torch.load(filename, map_location=self.device)\n",
        "            self.model.load_state_dict(checkpoint['state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            print(\"=> loaded checkpoint '{}'\".format(filename))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(filename))\n",
        "\n",
        "    def save(self, filename):\n",
        "        state = {'state_dict': self.model.state_dict(), 'optimizer': self.optimizer.state_dict()}\n",
        "        torch.save(state, filename)\n",
        "\n",
        "    def set_device(self, device):\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self, data_train, data_valid, images_path, masks_path, transforms_img, transforms_masks, output_path, max_epoch, batch_size):\n",
        "        train_dataset = Spine_Dataset_Segmentation(data_train, images_path, masks_path, transforms_img, transforms_masks, output_path)\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False, drop_last=True)\n",
        "\n",
        "        valid_dataset = Spine_Dataset_Segmentation(data_valid, images_path, masks_path, transforms_img, transforms_masks, output_path)\n",
        "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False,drop_last=True)\n",
        "\n",
        "        train_best_loss = 1.0e99 # self.__validate(train_loader)\n",
        "        valid_best_loss = self.__validate(valid_loader)\n",
        "        for epoch in range(max_epoch):\n",
        "            train_loss = self.__train_epoch(train_loader)\n",
        "            if train_loss < train_best_loss:\n",
        "                train_best_loss = train_loss\n",
        "                self.save(os.path.join(OUTPUT_MODEL_PATH,'deeplabv3_training.pt'))\n",
        "                print('Epoch {}: New best training loss: {}'.format(epoch, train_best_loss))\n",
        "            else:\n",
        "                print('Epoch {}: Current training loss: {}. Best loss: {}'.format(epoch, train_loss, train_best_loss))\n",
        "\n",
        "            valid_loss = self.__validate(valid_loader)\n",
        "            if valid_loss < valid_best_loss:\n",
        "                valid_best_loss = valid_loss\n",
        "                self.save(os.path.join(OUTPUT_MODEL_PATH,'deeplabv3_validation.pt'))\n",
        "                print('Epoch {}: New best validation loss: {}'.format(epoch, valid_best_loss))\n",
        "            else:\n",
        "                print('Epoch {}: Current validation loss: {}. Best loss: {}'.format(epoch, valid_loss, valid_best_loss))\n",
        "\n",
        "    def __train_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        torch.enable_grad()\n",
        "        train_loss = 0\n",
        "        count = len(train_loader)\n",
        "        for i, (batch, label) in enumerate(tqdm(train_loader)):\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(batch.to(self.device))\n",
        "            loss = self.criterion(output['out'].squeeze(1), label.to(self.device))\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if (i + 1) % self.print_every == 0:\n",
        "                tqdm.write('Sample {}/{}. Current training loss: {} '.format(i + 1, count, train_loss / i))\n",
        "        print()\n",
        "        train_loss /= count\n",
        "        return train_loss\n",
        "\n",
        "    def __validate(self, valid_loader):\n",
        "        self.model.eval()\n",
        "        torch.no_grad()\n",
        "        eval_loss = 0\n",
        "        count = len(valid_loader)\n",
        "        for i, (batch, label) in enumerate(tqdm(valid_loader)):\n",
        "            output = self.model(batch.to(self.device))\n",
        "            loss = self.criterion(output['out'].squeeze(1), label.to(self.device))\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            if (i + 1) % self.print_every == 0:\n",
        "                tqdm.write('Sample {}/{}. Current validation loss: {}'.format(i + 1, count, eval_loss / i))\n",
        "        print()\n",
        "        eval_loss /= count\n",
        "        return eval_loss\n",
        "\n",
        "    def test(self, data_test, images_path, masks_path, transforms_img, transforms_masks, output_path):\n",
        "        utils.remove_images_from_path(output_path)\n",
        "        test_dataset = Spine_Dataset_Segmentation(data_test, images_path, masks_path, transforms_img, transforms_masks, output_path)\n",
        "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "        self.model.eval()\n",
        "        torch.no_grad()\n",
        "        count = len(test_loader)\n",
        "        for i, (batch, _) in enumerate(tqdm(test_loader)):\n",
        "            output = self.model(batch.to(self.device))\n",
        "\n",
        "            with torch.no_grad():\n",
        "             if torch.cuda.is_available():\n",
        "              a = self.model(batch.to(self.device).type(torch.cuda.FloatTensor)/255)\n",
        "             else:\n",
        "              a = self.model(batch.to(device).type(torch.FloatTensor)/255)\n",
        "            outImage = a['out'].cpu().detach().numpy()[0]\n",
        "            cv2_imshow(outImage.transpose(1,2,0))\n",
        "\n",
        "            test_dataset.save_output(a, \\\n",
        "                    os.path.join(output_path, test_dataset.get_filename(i)))\n",
        "\n",
        "            if (i + 1) % self.print_every == 0:\n",
        "                tqdm.write('Sample {}/{}'.format(i + 1, count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FvARfYLBzzYP",
        "outputId": "1e5f9020-016c-4df3-96ff-3b878d1ff2ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> loading checkpoint '/content/drive/MyDrive/TFM/models/deeplabv3_training.pt'\n",
            "=> loaded checkpoint '/content/drive/MyDrive/TFM/models/deeplabv3_training.pt'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/61 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=500x800 at 0x7F2BEF365290>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/61 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5b81cc527db6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_MODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"deeplabv3_training.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGES_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMASKS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_IMAGES_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-108d391f3158>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, data_test, images_path, masks_path, transforms_img, transforms_masks, output_path)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             test_dataset.save_output(a, \\\n\u001b[0;32m--> 114\u001b[0;31m                     os.path.join(output_path, test_dataset.get_filename(i)))\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/TFM/My_modules/Datasets.py\u001b[0m in \u001b[0;36msave_output\u001b[0;34m(self, tensor, filename)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mpred_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (OrderedDict([('out', tensor([[[[2.2285, 2.2285, 2.2285,  ..., 3.1632, 3.1632, 3.1632],\n          [2.2285, 2.2285, 2.2285,  ..., 3.1632, 3.1632, 3.1632],\n          [2.2285, 2.2285, 2.2285,  ..., 3.1632, 3.1632, 3.1632],\n          ...,\n          [2.0796, 2.0796, 2.0796,  ..., 3.2446, 3.2446, 3.2446],\n          [2.0796, 2.0796, 2.0796,  ..., 3.2446, 3.2446, 3.2446],\n          [2.0796, 2.0796, 2.0796,  ..., 3.2446, 3.2446, 3.2446]]]],\n       device='cuda:0')), ('aux', tensor([[[[14.3582, 14.3582, 14.3582,  ..., 12.0446, 12.0446, 12.0446],\n          [14.3582, 14.3582, 14.3582,  ..., 12.0446, 12.0446, 12.0446],\n          [14.3582, 14.3582, 14.3582,  ..., 12.0446, 12.0446, 12.0446],\n          ...,\n          [11.8886, 11.8886, 11.8886,  ..., 11.8801, 11.8801, 11.8801],\n          [11.8886, 11.8886, 11.8886,  ..., 11.8801, 11.8801, 11.8801],\n          [11.8886, 11.8886, 11.8886,  ..., 11.8801, 11.8801, 11.8801]],\n\n         [[-0.5198, -0.5198, -0.5198,  ..., -0.6866, -0.6866, -0.6866],\n          [-0.5198, -0.5198, -0.5198,  ..., -0.6866, -0.6866, -0.6866],\n          [-0.5198, -0.5198, -0.5198,  ..., -0.6866, -0.6866, -0.6866],\n          ...,\n          [-0.0886, -0.0886, -0.0886,  ...,  0.5434,  0.5434,  0.5434],\n          [-0.0886, -0.0886, -0.0886,  ...,  0.5434,  0.5434,  0.5434],\n          [-0.0886, -0.0886, -0.0886,  ...,  0.5434,  0.5434,  0.5434]],\n\n         [[-2.2227, -2.2227, -2.2227,  ..., -2.0465, -2.0465, -2.0465],\n          [-2.2227, -2.2227, -2.2227,  ..., -2.0465, -2.0465, -2.0465],\n          [-2.2227, -2.2227, -2.2227,  ..., -2.0465, -2.0465, -2.0465],\n          ...,\n          [-2.0006, -2.0006, -2.0006,  ..., -1.6121, -1.6121, -1.6121],\n          [-2.0006, -2.0006, -2.0006,  ..., -1.6121, -1.6121, -1.6121],\n          [-2.0006, -2.0006, -2.0006,  ..., -1.6121, -1.6121, -1.6121]],\n\n         ...,\n\n         [[ 1.8647,  1.8647,  1.8647,  ...,  0.9457,  0.9457,  0.9457],\n          [ 1.8647,  1.8647,  1.8647,  ...,  0.9457,  0.9457,  0.9457],\n          [ 1.8647,  1.8647,  1.8647,  ...,  0.9457,  0.9457,  0.9457],\n          ...,\n          [ 2.1151,  2.1151,  2.1151,  ...,  1.4365,  1.4365,  1.4365],\n          [ 2.1151,  2.1151,  2.1151,  ...,  1.4365,  1.4365,  1.4365],\n          [ 2.1151,  2.1151,  2.1151,  ...,  1.4365,  1.4365,  1.4365]],\n\n         [[ 1.3753,  1.3753,  1.3753,  ...,  1.5222,  1.5222,  1.5222],\n          [ 1.3753,  1.3753,  1.3753,  ...,  1.5222,  1.5222,  1.5222],\n          [ 1.3753,  1.3753,  1.3753,  ...,  1.5222,  1.5222,  1.5222],\n          ...,\n          [ 1.1476,  1.1476,  1.1476,  ...,  1.0326,  1.0326,  1.0326],\n          [ 1.1476,  1.1476,  1.1476,  ...,  1.0326,  1.0326,  1.0326],\n          [ 1.1476,  1.1476,  1.1476,  ...,  1.0326,  1.0326,  1.0326]],\n\n         [[ 0.1105,  0.1105,  0.1105,  ...,  1.2401,  1.2401,  1.2401],\n          [ 0.1105,  0.1105,  0.1105,  ...,  1.2401,  1.2401,  1.2401],\n          [ 0.1105,  0.1105,  0.1105,  ...,  1.2401,  1.2401,  1.2401],\n          ...,\n          [-2.1864, -2.1864, -2.1864,  ...,  0.0984,  0.0984,  0.0984],\n          [-2.1864, -2.1864, -2.1864,  ...,  0.0984,  0.0984,  0.0984],\n          [-2.1864, -2.1864, -2.1864,  ...,  0.0984,  0.0984,  0.0984]]]],\n       device='cuda:0'))])) with an unsupported type (<class 'collections.OrderedDict'>) to a Tensor."
          ]
        }
      ],
      "source": [
        "net = NetManager()\n",
        "if False:\n",
        "  net.load(\"files_name\")\n",
        "max_epoch=10\n",
        "\n",
        "# Train=True\n",
        "Train = False\n",
        "if Train:\n",
        "  #net.load(os.path.join(OUTPUT_MODEL_PATH, \"deeplabv3_training.pt\"))\n",
        "  net.train(X_train, X_val, IMAGES_PATH, MASKS_PATH, transforms_img, transforms_masks, OUTPUT_IMAGES_PATH, max_epoch, BATCH_SIZE)\n",
        "\n",
        "else:\n",
        "  net.load(os.path.join(OUTPUT_MODEL_PATH, \"deeplabv3_training.pt\"))\n",
        "  net.test(X_test, IMAGES_PATH, MASKS_PATH, transforms_img, transforms_masks, OUTPUT_IMAGES_PATH)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "model_path = os.path.join(OUTPUT_MODEL_PATH, \"deeplabv3_training.pt\")\n",
        "image_path = IMAGES_PATH\n",
        "\n",
        "# Load the trained model\n",
        "if torch.cuda.is_available():\n",
        "    model = torch.load(model_path)\n",
        "else:\n",
        "    model = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    \n",
        "print(device)\n",
        "# Set the model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "# Read  a sample image and mask from the data-set\n",
        "for i in X_test:\n",
        "  originalImage = cv2.imread(os.path.join(image_path,i))\n",
        "\n",
        "  # Resize image\n",
        "  img = cv2.resize(originalImage, (800, 500), cv2.INTER_AREA).transpose(2,0,1)\n",
        "\n",
        "  img = img.reshape(1, 3, img.shape[1],img.shape[2])\n",
        "\n",
        "  start_time = time.time()\n",
        "  with torch.no_grad():\n",
        "      if torch.cuda.is_available():\n",
        "        a = model(torch.from_numpy(img).to(device).type(torch.cuda.FloatTensor)/255)\n",
        "      else:\n",
        "          a = model(torch.from_numpy(img).to(device).type(torch.FloatTensor)/255)\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "  outImage = a['out'].cpu().detach().numpy()[0]\n",
        "  plt.imshow(outImage.transpose(1,2,0))\n",
        "  break"
      ],
      "metadata": {
        "id": "g7L0_w4_iHL_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "1IyAg15ekfTUAGiGeD7VPbB8nJeDWxU-2",
      "authorship_tag": "ABX9TyO7hebSGx8nwaUz3dS655Iv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}